\relax 
\citation{graves2013speech}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Wielowarstwowa sie\IeC {\'c} neuronowa, kt\IeC {\'o}ra zawiera $(L+1)$-wastw z $D$ wej\IeC {\'s}ciami oraz $C$ wyj\IeC {\'s}ciami Network graph for a $(L+1)$-layer perceptron.}}{1}}
\newlabel{fig:multilayer-perceptron}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Regularizing RNNs with LSTM cells}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Long-short term memory units}{1}}
\newlabel{sec:lstm}{{1.1}{1}}
\citation{graves2013generating}
\citation{graves2013generating}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A graphical representation of LSTM memory cells used in this paper (there are minor differences in comparison to Graves \cite  {graves2013generating}).}}{2}}
\newlabel{fig:lstm}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Regularization with Dropout}{2}}
\newlabel{sec:reg}{{1.2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Regularized multilayer RNN. The dashed arrows indicate connections where dropout is applied, and the solid lines indicate connections where dropout is not applied.}}{3}}
\newlabel{fig:reg}{{3}{3}}
\citation{marcus1993building}
\citation{pascanu2013construct}
\citation{chenglanguage}
\citation{mikolov2012statistical}
\citation{chenglanguage}
\citation{mikolov2012context}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The thick line shows a typical path of information flow in the LSTM. The information is affected by dropout $L + 1$ times, where $L$ is depth of network.}}{4}}
\newlabel{fig:flow}{{4}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Experiments}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Language modeling}{4}}
\newlabel{sec:lang}{{2.1}{4}}
\citation{mikolov2010recurrent}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Word-level perplexity on the Penn Tree Bank dataset.}}{5}}
\newlabel{tab:ptb}{{1}{5}}
\citation{BourlardASR}
\citation{sak2014speech}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Some interesting samples drawn from a large regularized model conditioned on ``The meaning of life is''. We have removed ``unk'', ``N'', ``\$'' from the set of permissible words.}}{6}}
\newlabel{fig:meaning}{{5}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Speech recognition}{6}}
\newlabel{sec:speech}{{2.2}{6}}
\citation{sutskever2014sequence}
\citation{cho2014learning}
\citation{wmt_joint}
\citation{lium}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Frame-level accuracy on the Icelandic Speech Dataset. The training set has 93k utterances.}}{7}}
\newlabel{tab:speech}{{2}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Results on the English to French translation task. }}{7}}
\newlabel{tab:mt}{{3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Machine translation}{7}}
\newlabel{sec:trans}{{2.3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Image Caption Generation}{7}}
\newlabel{sec:caption}{{2.4}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Results on the image caption generation task. }}{7}}
\newlabel{tab:vis}{{4}{7}}
